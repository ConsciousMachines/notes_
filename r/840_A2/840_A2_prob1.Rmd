---
title: "GRADUATE STUDENT STAT 840 A2"
author: "Vsevolod Ladtchenko 20895137"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1

Since $X$ has a lognormal distribution, and $\epsilon$ has a normal distribution, and $Y$ is a function of $(X, \epsilon)$ we can decompose the expectation of $Y/X$ into an expectation over $X$ and an expectation over $\epsilon$.

$$
\begin{aligned}
\mathbb{E} \biggl[ \frac{Y}{X} \biggr] &= \mathbb{E} \biggl[ \frac{\exp(9 + 3 \log X + \epsilon)}{X} \biggr] \\
&= \mathbb{E}_X \biggl[ \mathbb{E}_{\epsilon} \biggl[ \frac{\exp(9 + 3 \log X + \epsilon)}{X} \bigg| X = x\biggr] \biggr] \\
\mathbb{E}_{\epsilon} \biggl[ \frac{\exp(9 + 3 \log X + \epsilon)}{X} \bigg| X = x\biggr] &= \mathbb{E}_{\epsilon} \biggl[ \frac{ e^9 e^{3 \log X} e^{\epsilon} }{X} \bigg| X = x\biggr] \\
&= \mathbb{E}_{\epsilon} \biggl[ \frac{ e^9 (e^{\log X})^3 e^{\epsilon} }{X} \bigg| X = x\biggr] \\
&= \mathbb{E}_{\epsilon} \biggl[ \frac{ e^9 X^3 e^{\epsilon} }{X} \bigg| X = x\biggr] \\
&= \mathbb{E}_{\epsilon} \bigl[ e^9 X^2 e^{\epsilon}  \big| X = x\bigr] \\
&= e^9 X^2 \mathbb{E}_{\epsilon} \bigl[ e^{\epsilon} \bigr] \qquad \texttt{since X constant} \\
&= e^9 X^2 e^{0.5} \qquad \texttt{lnorm(0,1) mean = }e^{0.5} \\
&= e^{9.5} X^2 \\
\mathbb{E}_X \bigl[ e^{9.5} X^2 \bigr] \approx \hat{\theta}_{RB} &= \frac{1}{n} \sum_{i=1}^n  e^{9.5} X_i^2 \\
\mathbb{E} \biggl[ \frac{Y}{X} \biggr] \approx \hat{\theta}_{MC} &= \frac{1}{n} \sum_{i=1}^n \frac{Y_i}{X_i}\\
\end{aligned}
$$

```{r}
d_RB = function(x)
{
  return(exp(9.5) * (x^2))
}

d_MC = function(x, e)
{
  return(exp(9) * (x^2) * (exp(e)))
}

get_SE = function(x) # return variance of sample mean
{
  n = length(x)
  mu = sum(x) / n
  s_sq = sum((x - mu)^2) / ((n-1)*n) # notes 2 p 30
  return(sqrt(s_sq))
}

get_CI = function(xbar, se, conf=.95)
{
  a = 1 - conf
  L = xbar + qnorm(a/2)*se
  U = xbar + qnorm(1 - a/2)*se
  return(c(L,U))
}


n = 100000000
x = rlnorm(n, 0, 1)
e = rnorm(n, 0, 1)
y = exp(9 + 3*log(x) + e)

# RB
deltas_RB = d_RB(x)
theta_hat_RB = mean(deltas_RB)
SE_RB = get_SE(deltas_RB)
CI_RB = get_CI(theta_hat_RB, SE_RB)

# simple
deltas_MC = d_MC(x, e)
theta_hat_MC = mean(deltas_MC)
SE_MC = get_SE(deltas_MC)
CI_MC = get_CI(theta_hat_MC, SE_MC)

# whenever reporting a MC estimate, report n, SE of estimate, and CI
# Rao Blackwell
round(c(n=n, theta=theta_hat_RB, se=SE_RB, ci=CI_RB), 0)
# Simple MC
round(c(n=n, theta=theta_hat_MC, se=SE_MC, ci=CI_MC), 0)
```

We see that the Rao-Blackwellized estimator enjoys a much smaller variance / standard error, compared to simple Monte-Carlo. The means of the estimates are very similar, agreeing with the theoretical result that both estimators have the same expected value. 








